{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as FT\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
    "    if box_format == \"midpoint\":\n",
    "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "\n",
    "    if box_format == \"corners\":\n",
    "        box1_x1 = boxes_preds[..., 0:1]\n",
    "        box1_y1 = boxes_preds[..., 1:2]\n",
    "        box1_x2 = boxes_preds[..., 2:3]\n",
    "        box1_y2 = boxes_preds[..., 3:4]  # (N, 1)\n",
    "        box2_x1 = boxes_labels[..., 0:1]\n",
    "        box2_y1 = boxes_labels[..., 1:2]\n",
    "        box2_x2 = boxes_labels[..., 2:3]\n",
    "        box2_y2 = boxes_labels[..., 3:4]\n",
    "\n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "\n",
    "    # .clamp(0) is for the case when they do not intersect\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "\n",
    "    return intersection / (box1_area + box2_area - intersection + 1e-6)\n",
    "\n",
    "\n",
    "def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n",
    "    assert type(bboxes) == list\n",
    "\n",
    "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "    bboxes_after_nms = []\n",
    "\n",
    "    while bboxes:\n",
    "        chosen_box = bboxes.pop(0)\n",
    "\n",
    "        bboxes = [\n",
    "            box\n",
    "            for box in bboxes\n",
    "            if box[0] != chosen_box[0]\n",
    "            or intersection_over_union(\n",
    "                torch.tensor(chosen_box[2:]),\n",
    "                torch.tensor(box[2:]),\n",
    "                box_format=box_format,\n",
    "            )\n",
    "            < iou_threshold\n",
    "        ]\n",
    "\n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "\n",
    "    return bboxes_after_nms\n",
    "\n",
    "\n",
    "def mean_average_precision(\n",
    "    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20\n",
    "):\n",
    "    # list storing all AP for respective classes\n",
    "    average_precisions = []\n",
    "\n",
    "    # used for numerical stability later on\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "\n",
    "        # Go through all predictions and targets,\n",
    "        # and only add the ones that belong to the\n",
    "        # current class c\n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "\n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(true_box)\n",
    "\n",
    "        # find the amount of bboxes for each training example\n",
    "        # Counter here finds how many ground truth bboxes we get\n",
    "        # for each training example, so let's say img 0 has 3,\n",
    "        # img 1 has 5 then we will obtain a dictionary with:\n",
    "        # amount_bboxes = {0:3, 1:5}\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "\n",
    "        # We then go through each key, val in this dictionary\n",
    "        # and convert to the following (w.r.t same example):\n",
    "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        # sort by box probabilities which is index 2\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "        \n",
    "        # If none exists for this class then we can safely skip\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            # Only take out the ground_truths that have the same\n",
    "            # training idx as detection\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            num_gts = len(ground_truth_img)\n",
    "            best_iou = 0\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = intersection_over_union(\n",
    "                    torch.tensor(detection[3:]),\n",
    "                    torch.tensor(gt[3:]),\n",
    "                    box_format=box_format,\n",
    "                )\n",
    "\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                # only detect ground truth detection once\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    # true positive and add this bounding box to seen\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "\n",
    "            # if IOU is lower then the detection is a false positive\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        # torch.trapz for numerical integration\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "    return sum(average_precisions) / len(average_precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBLock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.layer_norm = nn.LayerNorm(self.in_channels)\n",
    "        self.conv = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, **kwargs)\n",
    "        self.lrelu = nn.LeakyReLU(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.lrelu(x)\n",
    "        return x\n",
    "\n",
    "class YOLO(nn.Module):\n",
    "    def __init__(self, split_size, num_boxes, num_classes):\n",
    "        super().__init__()\n",
    "        self.S = split_size\n",
    "        self.B = num_boxes\n",
    "        self.num_classes = num_classes\n",
    "        self.cnn = resnet18(weights='DEFAULT')\n",
    "        self.cnn = nn.Sequential(*list(self.cnn.modules())[:-2])\n",
    "\n",
    "        '''\n",
    "        self.cnn = nn.Sequential(*[\n",
    "            CNNBLock(in_channels=3, out_channels=64, kernel_size=7, stride=2),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            CNNBLock(in_channels=64, out_channels=192, kernel_size=3),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            CNNBLock(in_channels=192, out_channels=128, kernel_size=1),\n",
    "            CNNBLock(in_channels=128, out_channels=256, kernel_size=3),\n",
    "            CNNBLock(in_channels=256, out_channels=256, kernel_size=1),\n",
    "            CNNBLock(in_channels=256, out_channels=512, kernel_size=3),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            \n",
    "            nn.Sequential(*([\n",
    "                CNNBLock(in_channels=512, out_channels=256, kernel_size=1),\n",
    "                CNNBLock(in_channels=256, out_channels=512, kernel_size=3),\n",
    "                ] * 4)),\n",
    "            CNNBLock(in_channels=512, out_channels=512, kernel_size=1),\n",
    "            CNNBLock(in_channels=512, out_channels=1024, kernel_size=3),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Sequential(*([\n",
    "                nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=1),\n",
    "                nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3),\n",
    "            ] * 2)),\n",
    "            CNNBLock(in_channels=1024, out_channels=1024, kernel_size=3),\n",
    "            CNNBLock(in_channels=1024, out_channels=1024, kernel_size=3, stride=2),\n",
    "\n",
    "            CNNBLock(in_channels=1024, out_channels=1024, kernel_size=3),\n",
    "            CNNBLock(in_channels=1024, out_channels=1024, kernel_size=3),\n",
    "        ])\n",
    "        '''\n",
    "\n",
    "        self.conn = nn.Sequential(*[\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=512, out_features=496),\n",
    "            nn.BatchNorm1d(496),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=496, out_features=self.S * self.S * (self.num_classes + self.B * 5)),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        print('tensor shape', x.shape)\n",
    "        x = self.cnn(x)\n",
    "        print('tensor shape', x.shape)\n",
    "        x = self.conn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLO_LOSS(nn.Module):\n",
    "    def __init__(self, split_size, num_boxes, num_classes, lambda_coor, lambda_no_obj):\n",
    "        super().__init__()\n",
    "        self.C = num_classes\n",
    "        self.S = split_size\n",
    "        self.B = num_boxes\n",
    "        self.lambda_coor = lambda_coor\n",
    "        self.lambda_no_obj = lambda_no_obj\n",
    "\n",
    "        self.mse = nn.MSELoss(reduction='sum')\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        predictions = predictions.view(-1, self.S, self.S, self.C + self.B * 5)\n",
    "        ious = []\n",
    "        for i in range(self.B):\n",
    "            print(predictions.shape)\n",
    "            ious.append(intersection_over_union(predictions[..., (self.C + i * self.B):(self.C + 5 + i * self.B)], targets[..., (self.C + i * self.B):(self.C + 5 + i * self.B)]))\n",
    "        ious = torch.cat([iou.unsqueeze(0) for iou in ious], dim=0)\n",
    "        ious_maxes, bestbox = torch.max(ious, dim=0)\n",
    "\n",
    "        # 1_obj_i\n",
    "        exists_box = targets[..., self.C].unsqueeze(3)\n",
    "\n",
    "        # loss for box coordinates\n",
    "        box_predictions = exists_box * (bestbox * predictions[..., 26:30] + (1 - bestbox) * predictions[..., 21:25])\n",
    "        box_targets = exists_box * targets[..., 21:25]\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * (torch.sqrt(torch.abs(box_predictions[..., 2:4])) + 1e-6)\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "        box_loss = self.mse(torch.flatten(box_predictions, end_dim=-2), torch.flatten(box_targets, end_dim=-2))\n",
    "\n",
    "        # loss for objects\n",
    "        pred_box = (\n",
    "            bestbox * predictions[..., 25:26] + (1 - bestbox) * predictions[..., 20:21]\n",
    "        )\n",
    "\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(exists_box * pred_box),\n",
    "            torch.flatten(exists_box * targets[..., 20:21]),\n",
    "        )\n",
    "\n",
    "        # loss for  no objects\n",
    "\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 20:21], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * targets[..., 20:21], start_dim=1),\n",
    "        )\n",
    "\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 25:26], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * targets[..., 20:21], start_dim=1)\n",
    "        )\n",
    "\n",
    "        # loss for classes\n",
    "\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box * predictions[..., :20], end_dim=-2,),\n",
    "            torch.flatten(exists_box * targets[..., :20], end_dim=-2,),\n",
    "        )\n",
    "\n",
    "        loss = (\n",
    "            self.lambda_coor * box_loss  # first two rows in paper\n",
    "            + object_loss  # third row in paper\n",
    "            + self.lambda_no_obj * no_object_loss  # forth row\n",
    "            + class_loss  # fifth row\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, label_dir, split_size=7, num_boxes=2, num_classes=20, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform=transform\n",
    "        self.B = num_boxes\n",
    "        self.S = split_size\n",
    "        self.C = num_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label_path = os.path.join(self.label_dir, self.annotations.iloc[idx, 1])\n",
    "        boxes = []\n",
    "        with open(label_path) as label_file:\n",
    "            for label in label_file.readlines():\n",
    "                class_label, x, y, w, h = [\n",
    "                    float(x) if float(x) != int(float(x)) else int(x)\n",
    "                    for x in label.replace(\"\\n\", \"\").split()\n",
    "                ]\n",
    "                boxes.append([class_label, x, y, w, h])\n",
    "    \n",
    "        img_path = os.path.join(self.img_dir, self.annotations.iloc[idx, 0])\n",
    "        image = Image.open(img_path)\n",
    "        boxes = torch.tensor(boxes)\n",
    "\n",
    "        if self.transform != None:\n",
    "            image, boxes = self.transform(image, boxes)\n",
    "\n",
    "        label_matrix = torch.zeros(self.S, self.S, self.C + self.B * 5)\n",
    "        for box in boxes:\n",
    "            class_label, x, y, w, h = box.tolist()\n",
    "            class_label = int(class_label)\n",
    "            i, j = int(self.S * y), int(self.S * x)\n",
    "            x_cell, y_cell = self.S * x - j, - self.S * y - i\n",
    "            w_cell, h_cell = w * self.S, h * self.S\n",
    "\n",
    "            if label_matrix[i, j, self.C] == 0:\n",
    "                label_matrix[i, j, self.C] = 1\n",
    "                box_coordinates = torch.tensor([x_cell, y_cell, w_cell, h_cell])\n",
    "                label_matrix[i, j, (self.C + 1):(self.C + 5)] = box_coordinates\n",
    "                label_matrix[i, j, class_label] = 1\n",
    "\n",
    "        return image, label_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cellboxes(predictions, S=7):\n",
    "    predictions = predictions.to(\"cpu\")\n",
    "    batch_size = predictions.shape[0]\n",
    "    predictions = predictions.reshape(batch_size, 7, 7, 30)\n",
    "    bboxes1 = predictions[..., 21:25]\n",
    "    bboxes2 = predictions[..., 26:30]\n",
    "    scores = torch.cat(\n",
    "        (predictions[..., 20].unsqueeze(0), predictions[..., 25].unsqueeze(0)), dim=0\n",
    "    )\n",
    "    best_box = scores.argmax(0).unsqueeze(-1)\n",
    "    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n",
    "    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n",
    "    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n",
    "    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n",
    "    w_y = 1 / S * best_boxes[..., 2:4]\n",
    "    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n",
    "    predicted_class = predictions[..., :20].argmax(-1).unsqueeze(-1)\n",
    "    best_confidence = torch.max(predictions[..., 20], predictions[..., 25]).unsqueeze(\n",
    "        -1\n",
    "    )\n",
    "    converted_preds = torch.cat(\n",
    "        (predicted_class, best_confidence, converted_bboxes), dim=-1\n",
    "    )\n",
    "\n",
    "    return converted_preds\n",
    "\n",
    "def cellboxes_to_boxes(out, S=7):\n",
    "    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n",
    "    converted_pred[..., 0] = converted_pred[..., 0].long()\n",
    "    all_bboxes = []\n",
    "\n",
    "    for ex_idx in range(out.shape[0]):\n",
    "        bboxes = []\n",
    "\n",
    "        for bbox_idx in range(S * S):\n",
    "            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n",
    "        all_bboxes.append(bboxes)\n",
    "\n",
    "    return all_bboxes\n",
    "\n",
    "def get_bboxes(\n",
    "    loader,\n",
    "    model,\n",
    "    iou_threshold,\n",
    "    threshold,\n",
    "    pred_format=\"cells\",\n",
    "    box_format=\"midpoint\",\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    all_pred_boxes = []\n",
    "    all_true_boxes = []\n",
    "\n",
    "    # make sure model is in eval before get bboxes\n",
    "    model.eval()\n",
    "    train_idx = 0\n",
    "\n",
    "    for batch_idx, (x, labels) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            print(\"x shape in getbboxes\", x.shape)\n",
    "            predictions = model(x)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        true_bboxes = cellboxes_to_boxes(labels)\n",
    "        bboxes = cellboxes_to_boxes(predictions)\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            nms_boxes = non_max_suppression(\n",
    "                bboxes[idx],\n",
    "                iou_threshold=iou_threshold,\n",
    "                threshold=threshold,\n",
    "                box_format=box_format,\n",
    "            )\n",
    "\n",
    "            for nms_box in nms_boxes:\n",
    "                all_pred_boxes.append([train_idx] + nms_box)\n",
    "\n",
    "            for box in true_bboxes[idx]:\n",
    "                # many will get converted to 0 pred\n",
    "                if box[1] > threshold:\n",
    "                    all_true_boxes.append([train_idx] + box)\n",
    "\n",
    "            train_idx += 1\n",
    "\n",
    "    model.train()\n",
    "    return all_pred_boxes, all_true_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters etc. \n",
    "LEARNING_RATE = 2e-5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "BATCH_SIZE = 4 # 64 in original paper but I don't have that much vram, grad accum?\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 1000\n",
    "NUM_WORKERS = 1\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "LOAD_MODEL_FILE = \"overfit.pth.tar\"\n",
    "IMG_DIR = \"data/images\"\n",
    "LABEL_DIR = \"data/labels\"\n",
    "\n",
    "print(DEVICE)\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, bboxes):\n",
    "        for t in self.transforms:\n",
    "            img, bboxes = t(img), bboxes\n",
    "\n",
    "        return img, bboxes\n",
    "\n",
    "\n",
    "transform = Compose([transforms.Resize((448, 448)), transforms.ToTensor(),])\n",
    "\n",
    "\n",
    "def train_fn(train_loader, model, optimizer, loss_fn):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    mean_loss = []\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "        mean_loss.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update progress bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Mean loss was {sum(mean_loss)/len(mean_loss)}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    model = YOLO(split_size=7, num_boxes=2, num_classes=20).to(DEVICE)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    loss_fn = YOLO_LOSS(split_size=7, num_boxes=2, num_classes=20, lambda_coor=5, lambda_no_obj=0.5)\n",
    "\n",
    "    train_dataset = VOCDataset(\n",
    "        \"data/100examples.csv\",\n",
    "        transform=transform,\n",
    "        img_dir=IMG_DIR,\n",
    "        label_dir=LABEL_DIR,\n",
    "    )\n",
    "\n",
    "    test_dataset = VOCDataset(\n",
    "        \"data/8examples.csv\", \n",
    "        transform=transform, \n",
    "        img_dir=IMG_DIR, \n",
    "        label_dir=LABEL_DIR,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        #num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    print('train loaded')\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        #num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    print('test loaded')\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print('epoch')\n",
    "        pred_boxes, target_boxes = get_bboxes(\n",
    "            train_loader, model, iou_threshold=0.5, threshold=0.4\n",
    "        )\n",
    "\n",
    "        print('pred_boxes')\n",
    "\n",
    "        mean_avg_prec = mean_average_precision(\n",
    "            pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n",
    "        )\n",
    "\n",
    "        print('mean average precision')\n",
    "\n",
    "        print(f\"Train mAP: {mean_avg_prec}\")\n",
    "        train_fn(train_loader, model, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
